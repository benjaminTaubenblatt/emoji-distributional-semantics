{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import emoji\n",
    "import string\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre> \n",
    "\n",
    "The following methods are only used to clean and save the original data and should not be run\n",
    "    \n",
    "    <b>The cleaned data can be found in emoji_datasets/all_data.csv</b>\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data\n",
    "\n",
    "def load_test_data(file_path):\n",
    "    with open(file_path) as fp:\n",
    "        result = []\n",
    "        translator = str.maketrans('', '', string.punctuation)\n",
    "        line = fp.readline()\n",
    "        while line:\n",
    "            line = line.strip()\n",
    "            line = line.split(',',1)\n",
    "            if len(line) == 2:\n",
    "                clean_row1 = line[1].translate(translator)\n",
    "                clean_row2 = clean_row1.replace(chr(8220),'')\n",
    "                clean_row3 = clean_row2.replace(chr(8221),'')\n",
    "                line = [line[0], clean_row3]\n",
    "                value = np.array([line[0], line[1]])\n",
    "                result.append(value)\n",
    "            line = fp.readline()\n",
    "        return pd.DataFrame(np.array(result, dtype='object'))\n",
    "    \n",
    "def extract_emojis(example):\n",
    "    return (' '.join(c for c in example if c in emoji.UNICODE_EMOJI)).split()\n",
    "\n",
    "def prune_dataset_emojis(data):\n",
    "    result = []\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    for i,row in enumerate(data):\n",
    "        try:\n",
    "            if extract_emojis(row[1]) != []:\n",
    "                clean_row1 = row[1].translate(translator)\n",
    "                clean_row2 = clean_row1.replace(chr(8220),'')\n",
    "                clean_row3 = clean_row2.replace(chr(8221),'')\n",
    "                new_row = np.array([row[0], clean_row3])\n",
    "                result.append(new_row)\n",
    "        except TypeError:\n",
    "            pass\n",
    "    return pd.DataFrame(np.array(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean data and write to CSV\n",
    "\n",
    "train_data_raw = pd.read_csv('emoji_datasets/data_train.csv', header=None, encoding='utf-8')\n",
    "\n",
    "test_data_raw = load_test_data('emoji_datasets/data_test.txt')\n",
    "\n",
    "train_data_clean = prune_dataset_emojis(train_data_raw.values)\n",
    "test_data_clean = prune_dataset_emojis(test_data_raw.values)\n",
    "\n",
    "all_data_clean_np = np.vstack((train_data_clean.values, test_data_clean.values))\n",
    "np.random.shuffle(all_data_clean_np)\n",
    "all_data_clean = pd.DataFrame(all_data_clean_np)\n",
    "\n",
    "all_data_clean.to_csv('emoji_datasets/all_data.csv', header=None, index=False, encoding='utf-8', quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre> END : clean data </pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_and_labels_RAW = pd.read_csv('emoji_datasets/all_data.csv', header=None, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_emojis(example):\n",
    "    result = []\n",
    "    ptr = 0\n",
    "    for i,c in enumerate(example):\n",
    "        if c in emoji.UNICODE_EMOJI:\n",
    "            split = example[ptr:i]\n",
    "            if split != '':\n",
    "                result.append(split)\n",
    "                result.append(c)\n",
    "            else:\n",
    "                result.append(c)\n",
    "            ptr = i+1\n",
    "    return result\n",
    "\n",
    "def preprocess(data):\n",
    "    labels = list(data[:,0])\n",
    "    tweets = list(data[:,1])\n",
    "    result = []\n",
    "    tweet_tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True)\n",
    "    for i,twt in enumerate(tweets):\n",
    "        clean_tokens = []\n",
    "        tokens = tweet_tokenizer.tokenize(twt)\n",
    "        for j,tk in enumerate(tokens):\n",
    "            tk = tk.lower()\n",
    "            sep = separate_emojis(tk)\n",
    "            if sep != []:\n",
    "                clean_tokens = clean_tokens + sep\n",
    "            else:\n",
    "                clean_tokens.append(tk)\n",
    "        result.append((labels[i], clean_tokens))\n",
    "    return result\n",
    "\n",
    "def find_all_emojis(data):\n",
    "    emoji_dict = defaultdict(int)\n",
    "    for twt in data:\n",
    "        for word in twt[1]:\n",
    "            if word in emoji.UNICODE_EMOJI:\n",
    "                emoji_dict[word] += 1\n",
    "    return emoji_dict\n",
    "\n",
    "def term_context_matrix(targets, data):\n",
    "    tc_matrix = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "    for twt in data:\n",
    "        for w1 in targets:\n",
    "            if w1 in twt[1]:\n",
    "                for w2 in twt[1]:\n",
    "                    tc_matrix[w1][w2]+=1\n",
    "    return tc_matrix\n",
    "\n",
    "def vocab_map(dd):\n",
    "    vocab = {}\n",
    "    vocab_id = 0\n",
    "    for k,v in dd.items():\n",
    "        for k2,v2 in v.items():\n",
    "            if k2 not in vocab.keys():\n",
    "                vocab[k2] = vocab_id\n",
    "                vocab_id += 1\n",
    "    return vocab\n",
    "\n",
    "def term_to_int_dd(dd):\n",
    "    num_rows = len(dd.keys())\n",
    "    data = defaultdict(int)\n",
    "    vocab_dict = vocab_map(dd)\n",
    "    for i,r in enumerate(dd.keys()):\n",
    "        for j,c in enumerate(dd[r].keys()):\n",
    "            data[i,vocab_dict[c]] = dd[r][c]\n",
    "            \n",
    "    return data\n",
    "\n",
    "def term_to_sparse(dd):\n",
    "    dd_int = term_to_int_dd(dd)\n",
    "    vs = [v for (i,j), v in dd_int.items()]\n",
    "    ii = [i for (i,j), v in dd_int.items()]\n",
    "    jj = [j for (i,j), v in dd_int.items()]\n",
    "    matrix = coo_matrix((vs, (ii, jj)))\n",
    "    return matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_and_labels = preprocess(tweets_and_labels_RAW.values)\n",
    "emoji_counts = find_all_emojis(tweets_and_labels)\n",
    "emoji_targets = list(emoji_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_matrix = term_context_matrix(emoji_targets, tweets_and_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_coo_matrix = term_to_sparse(term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(606, 24726)"
      ]
     },
     "execution_count": 506,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_coo_matrix.toarray().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   5,  11, ...,   0,   0,   0],\n",
       "       [  1,   7,  11, ...,   0,   0,   0],\n",
       "       [  0, 174, 239, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0],\n",
       "       [  0,   0,   0, ...,   0,   0,   0]])"
      ]
     },
     "execution_count": 508,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emoji_coo_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24726"
      ]
     },
     "execution_count": 509,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Debugging methods\n",
    "\n",
    "def count_distinct_vocab(dd):\n",
    "    count = 0\n",
    "    seen = defaultdict(int)\n",
    "    for k,v in dd.items():\n",
    "        for k2,v2 in v.items():\n",
    "            if k2 not in seen.keys():\n",
    "                count +=1\n",
    "                seen[k2] = 1\n",
    "    return count\n",
    "\n",
    "def get_max_val(dd):\n",
    "    mx = 0\n",
    "    rest = 0\n",
    "    for k,v in dd.items():\n",
    "        for k2,v2 in v.items():\n",
    "            if v2 > mx:\n",
    "                mx = v2\n",
    "            else:\n",
    "                rest += v2\n",
    "    return mx, rest\n",
    "\n",
    "def get_max_keys(dd):\n",
    "    mx = 0\n",
    "    for k,v in dd.items():\n",
    "        for k2,v2 in v.items():\n",
    "            if len(k2) > mx:\n",
    "                mx = len(k2)\n",
    "    return mx\n",
    "\n",
    "count_distinct_vocab(term_matrix)\n",
    "#get_max_val(term_matrix)\n",
    "#get_max_keys(term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
